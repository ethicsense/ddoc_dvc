#!/usr/bin/env python3
"""
DVCì™€ ddoc ëª¨ë“ˆì„ í†µí•©í•œ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
DVCê°€ datasets ë³€ê²½ì„ ê°ì§€í•˜ë©´ ì‹¤í–‰ë˜ê³ , ddocì˜ ìºì‹œë¡œ ì¦ë¶„ ë¶„ì„
"""
import sys
import os
from pathlib import Path
from datetime import datetime
import json
import pickle
import yaml

# ddoc ëª¨ë“ˆ import
sys.path.insert(0, str(Path(__file__).parent.parent / 'datadrift_app_engine'))

try:
    from main import run_attribute_analysis_wrapper, run_embedding_analysis
    from cache_utils import get_cached_analysis_data, save_analysis_data
    print("âœ… ddoc ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ ddoc ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("datadrift_app_engineì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    sys.exit(1)

def validate_cache(data_dir, cache_data, formats):
    """ì‹¤ì œ íŒŒì¼ ê²€ì¦ ë° orphan cache ì œê±°"""
    if not cache_data:
        return cache_data, set(), 0
    
    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” íŒŒì¼ ëª©ë¡
    actual_files = set()
    for root, _, files in os.walk(data_dir):
        for file in files:
            if file.endswith(tuple(formats)):
                actual_files.add(file)
    
    # orphan cache ì œê±°
    cached_files = set(cache_data.keys())
    orphaned = cached_files - actual_files
    
    if orphaned:
        print(f"\nğŸ—‘ï¸  ì‚­ì œëœ íŒŒì¼ì˜ ìºì‹œ ì •ë¦¬: {len(orphaned)}ê°œ")
        for fname in orphaned:
            del cache_data[fname]
            print(f"   - {fname}")
    
    return cache_data, actual_files, len(orphaned)

def analyze_dataset(dataset_name=None):
    """ddoc ëª¨ë“ˆë¡œ ë°ì´í„°ì…‹ ë¶„ì„ (ë°ì´í„°ì…‹ë³„ ë…ë¦½ ê´€ë¦¬)
    
    Args:
        dataset_name: ë¶„ì„í•  ë°ì´í„°ì…‹ ì´ë¦„ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
    """
    
    # params.yaml ë¡œë“œ
    with open('params.yaml', 'r') as f:
        params = yaml.safe_load(f)
    
    # ë°ì´í„°ì…‹ ì •ë³´ ì¶”ì¶œ
    if dataset_name:
        # íŠ¹ì • ë°ì´í„°ì…‹ ì°¾ê¸°
        dataset_config = next(
            (ds for ds in params.get('datasets', []) if ds['name'] == dataset_name),
            None
        )
        if not dataset_config:
            print(f"âŒ ë°ì´í„°ì…‹ '{dataset_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)
        
        data_dir = dataset_config['path']
        formats = tuple(dataset_config['formats'])
    else:
        # ê¸°ë³¸ê°’ ì‚¬ìš© (í•˜ìœ„ í˜¸í™˜)
        data_dir = params['analysis']['data_dir']
        formats = tuple(params['analysis']['formats'])
    
    data_dir = Path(data_dir)
    dataset_name_only = data_dir.name  # "test_data"
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # ë¶„ì„ ê²°ê³¼ë¥¼ datasets ë°–ì˜ analysis/ ë””ë ‰í† ë¦¬ì— ì €ì¥
    analysis_root = Path("analysis") / dataset_name_only
    
    plot_dir = analysis_root / "plots"
    plot_dir.mkdir(parents=True, exist_ok=True)
    
    # drift ë””ë ‰í† ë¦¬ë„ ë¯¸ë¦¬ ìƒì„±
    drift_plot_dir = analysis_root / "drift" / "plots"
    drift_plot_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸš€ DVC + ddoc í†µí•© ë¶„ì„ ì‹œì‘")
    print(f"=" * 80)
    print(f"ì‹œê°„: {timestamp}")
    print(f"ë°ì´í„° ë””ë ‰í† ë¦¬: {data_dir}")
    print(f"ì§€ì› í˜•ì‹: {formats}")
    print()
    
    # ë©”íŠ¸ë¦­ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬
    metrics = {}
    
    # 1. ì†ì„± ë¶„ì„ (ddocì˜ í•´ì‹œ ê¸°ë°˜ ìºì‹± í™œìš©)
    print("ğŸ“Š Step 1: Attribute Analysis")
    print("-" * 80)
    
    # cache ë””ë ‰í† ë¦¬ëŠ” ddocê°€ ìë™ìœ¼ë¡œ ì œì™¸í•˜ë¯€ë¡œ ë³„ë„ ì²˜ë¦¬ ë¶ˆí•„ìš”
    attr_stats = run_attribute_analysis_wrapper([str(data_dir)], formats)
    
    # ddoc ìºì‹œì—ì„œ ì „ì²´ ê²°ê³¼ ë¡œë“œ
    attr_cache = get_cached_analysis_data(data_dir, "attribute_analysis")
    
    # ğŸ” ê²€ì¦: ì‹¤ì œ ì¡´ì¬í•˜ëŠ” íŒŒì¼ë§Œ ìœ ì§€
    if attr_cache:
        attr_cache, actual_files, orphaned_count = validate_cache(data_dir, attr_cache, formats)
        # ê²€ì¦ í›„ ìºì‹œ ì¬ì €ì¥ (orphan ì œê±°ë¨)
        if orphaned_count > 0:
            save_analysis_data(data_dir, attr_cache, "attribute_analysis")
            metrics["orphaned_files_removed"] = orphaned_count
    
    if attr_cache:
        num_files = len(attr_cache)
        sizes = [v['size'] for v in attr_cache.values() if 'size' in v]
        widths = [v['width'] for v in attr_cache.values() if 'width' in v]
        heights = [v['height'] for v in attr_cache.values() if 'height' in v]
        
        # ë©”íŠ¸ë¦­ ì €ì¥
        data_dir_key = str(data_dir)
        metrics["num_files"] = num_files
        metrics["avg_size_mb"] = sum(sizes) / len(sizes) if sizes else 0
        metrics["avg_width"] = sum(widths) / len(widths) if widths else 0
        metrics["avg_height"] = sum(heights) / len(heights) if heights else 0
        metrics["files_processed"] = attr_stats[data_dir_key]['processed_files']
        metrics["files_cached"] = attr_stats[data_dir_key]['skipped_files']
        
        print(f"âœ… ë¶„ì„ ì™„ë£Œ: {num_files}ê°œ íŒŒì¼")
        print(f"   ìƒˆë¡œ ë¶„ì„: {attr_stats[data_dir_key]['processed_files']}ê°œ")
        print(f"   ìºì‹œ í™œìš©: {attr_stats[data_dir_key]['skipped_files']}ê°œ")
        
        # ì‹œê°í™”: ì†ì„± ë¶„ì„ (ê°œë³„ ì°¨íŠ¸ë¡œ ì €ì¥)
        import matplotlib.pyplot as plt
        import numpy as np
        
        # í™”ì§ˆ ê´€ë ¨ ë°ì´í„° ì¶”ì¶œ
        noise_levels = [v['noise_level'] for v in attr_cache.values() if 'noise_level' in v]
        sharpness_vals = [v['sharpness'] for v in attr_cache.values() if 'sharpness' in v]
        
        # Quality Score ê³„ì‚°
        def calculate_quality_score(sharpness, noise_level):
            """ì¢…í•© í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0~100, ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)"""
            sharp_norm = min(sharpness / 100, 1.0)
            noise_norm = max(0, 1.0 - (noise_level / 50))
            quality = (sharp_norm * 0.6 + noise_norm * 0.4) * 100
            return quality
        
        quality_scores = []
        if noise_levels and sharpness_vals and len(noise_levels) == len(sharpness_vals):
            quality_scores = [calculate_quality_score(s, n) for s, n in zip(sharpness_vals, noise_levels)]
            metrics["avg_quality_score"] = sum(quality_scores) / len(quality_scores)
        
        # 1. íŒŒì¼ í¬ê¸° ë¶„í¬
        plt.figure(figsize=(10, 6))
        plt.hist(sizes, bins=20, color='skyblue', edgecolor='black', alpha=0.7)
        plt.title('File Size Distribution', fontsize=14, fontweight='bold')
        plt.xlabel('Size (MB)')
        plt.ylabel('Count')
        plt.grid(alpha=0.3)
        plt.tight_layout()
        plt.savefig(plot_dir / 'size_distribution.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. ë…¸ì´ì¦ˆ ë ˆë²¨ ë¶„í¬
        if noise_levels:
            plt.figure(figsize=(10, 6))
            plt.hist(noise_levels, bins=20, color='lightcoral', edgecolor='black', alpha=0.7)
            plt.title('Noise Level Distribution', fontsize=14, fontweight='bold')
            plt.xlabel('Noise Level')
            plt.ylabel('Count')
            plt.grid(alpha=0.3)
            plt.tight_layout()
            plt.savefig(plot_dir / 'noise_distribution.png', dpi=300, bbox_inches='tight')
            plt.close()
            metrics["avg_noise_level"] = sum(noise_levels) / len(noise_levels)
        
        # 3. ì„ ëª…ë„ ë¶„í¬
        if sharpness_vals:
            plt.figure(figsize=(10, 6))
            plt.hist(sharpness_vals, bins=20, color='lightgreen', edgecolor='black', alpha=0.7)
            plt.title('Sharpness Distribution', fontsize=14, fontweight='bold')
            plt.xlabel('Sharpness')
            plt.ylabel('Count')
            plt.grid(alpha=0.3)
            plt.tight_layout()
            plt.savefig(plot_dir / 'sharpness_distribution.png', dpi=300, bbox_inches='tight')
            plt.close()
            metrics["avg_sharpness"] = sum(sharpness_vals) / len(sharpness_vals)
        
        # 4. í’ˆì§ˆ ë§µ (ë…¸ì´ì¦ˆ vs ì„ ëª…ë„)
        if noise_levels and sharpness_vals:
            plt.figure(figsize=(10, 8))
            scatter = plt.scatter(noise_levels, sharpness_vals, 
                                alpha=0.6, s=100, c=sizes, cmap='viridis', edgecolors='black')
            plt.title('Quality Map: Noise vs Sharpness', fontsize=14, fontweight='bold')
            plt.xlabel('Noise Level')
            plt.ylabel('Sharpness')
            plt.colorbar(scatter, label='File Size (MB)')
            plt.grid(alpha=0.3)
            plt.tight_layout()
            plt.savefig(plot_dir / 'quality_map.png', dpi=300, bbox_inches='tight')
            plt.close()
        
        # 5. ì¢…í•© í’ˆì§ˆ ìŠ¤ì½”ì–´ ë¶„í¬
        if quality_scores:
            plt.figure(figsize=(10, 6))
            plt.hist(quality_scores, bins=20, color='gold', edgecolor='black', alpha=0.7)
            plt.title('Quality Score Distribution', fontsize=14, fontweight='bold')
            plt.xlabel('Quality Score (0-100)')
            plt.ylabel('Count')
            plt.axvline(30, color='red', linestyle='--', alpha=0.5, label='Poor')
            plt.axvline(50, color='orange', linestyle='--', alpha=0.5, label='Fair')
            plt.axvline(70, color='green', linestyle='--', alpha=0.5, label='Good')
            plt.legend()
            plt.grid(alpha=0.3)
            plt.tight_layout()
            plt.savefig(plot_dir / 'quality_score.png', dpi=300, bbox_inches='tight')
            plt.close()
        
        # 6. í•´ìƒë„ ë¶„í¬
        if widths and heights:
            resolutions = [w * h / 1000000 for w, h in zip(widths, heights)]
            plt.figure(figsize=(10, 6))
            plt.hist(resolutions, bins=20, color='lightblue', edgecolor='black', alpha=0.7)
            plt.title('Resolution Distribution', fontsize=14, fontweight='bold')
            plt.xlabel('Megapixels')
            plt.ylabel('Count')
            plt.grid(alpha=0.3)
            plt.tight_layout()
            plt.savefig(plot_dir / 'resolution_distribution.png', dpi=300, bbox_inches='tight')
            plt.close()
        
        print(f"   ğŸ“Š ì†ì„± ë¶„ì„ ì‹œê°í™” ì €ì¥: {plot_dir}/*.png (6ê°œ íŒŒì¼)")
    else:
        print("âš ï¸ ì†ì„± ë¶„ì„ ê²°ê³¼ ì—†ìŒ")
    
    print()
    
    # 2. ì„ë² ë”© ë¶„ì„ (ddocì˜ í•´ì‹œ ê¸°ë°˜ ìºì‹± í™œìš©)
    print("ğŸ”¬ Step 2: Embedding Analysis")
    print("-" * 80)
    
    # cache ë””ë ‰í† ë¦¬ëŠ” ddocê°€ ìë™ìœ¼ë¡œ ì œì™¸í•˜ë¯€ë¡œ ë³„ë„ ì²˜ë¦¬ ë¶ˆí•„ìš”
    emb_stats = run_embedding_analysis(
        [str(data_dir)], 
        formats,
        model=params['embedding']['model'],
        device=params['embedding']['device'],
        n_clusters=params['clustering']['n_clusters'],
        method=params['clustering']['method'],
        cluster_selection_method=params['clustering']['selection_method']
    )
    
    # ddoc ìºì‹œì—ì„œ ì„ë² ë”© ê²°ê³¼ ë¡œë“œ
    emb_cache = get_cached_analysis_data(data_dir, "embedding_analysis")
    
    if emb_cache:
        embeddings = [v['embedding'] for v in emb_cache.values() if 'embedding' in v]
        
        metrics["num_embeddings"] = len(embeddings)
        metrics["embedding_dim"] = len(embeddings[0]) if embeddings else 0
        
        print(f"âœ… ì„ë² ë”© ì™„ë£Œ: {len(embeddings)}ê°œ")
        
        # ì‹œê°í™”: PCA 3D
        if len(embeddings) > 1:
            from sklearn.decomposition import PCA
            from mpl_toolkits.mplot3d import Axes3D
            
            emb_array = np.array(embeddings)
            pca = PCA(n_components=3)
            emb_3d = pca.fit_transform(emb_array)
            
            fig = plt.figure(figsize=(12, 9))
            ax = fig.add_subplot(111, projection='3d')
            
            scatter = ax.scatter(emb_3d[:, 0], emb_3d[:, 1], emb_3d[:, 2],
                                alpha=0.6, s=100, c=range(len(emb_3d)), 
                                cmap='viridis', edgecolors='black', linewidth=0.5)
            
            ax.set_title(f'Embedding Space (PCA 3D)\nVariance: {pca.explained_variance_ratio_.sum():.1%}', 
                        fontsize=14, fontweight='bold')
            ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
            ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
            ax.set_zlabel(f'PC3 ({pca.explained_variance_ratio_[2]:.1%})')
            
            plt.colorbar(scatter, label='Sample Index', pad=0.1)
            plt.tight_layout()
            plt.savefig(plot_dir / 'embedding_pca_3d.png', dpi=300, bbox_inches='tight')
            plt.close()
            print(f"   ğŸ“Š ì„ë² ë”© 3D ì‹œê°í™” ì €ì¥: {plot_dir / 'embedding_pca_3d.png'}")
    else:
        print("âš ï¸ ì„ë² ë”© ë¶„ì„ ê²°ê³¼ ì—†ìŒ")
    
    print()
    
    # 3. í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„
    print("ğŸ¯ Step 3: Clustering Analysis")
    print("-" * 80)
    
    cluster_cache = get_cached_analysis_data(data_dir, "clustering_analysis")
    
    if cluster_cache:
        n_clusters = cluster_cache.get('n_clusters', 0)
        
        metrics["num_clusters"] = n_clusters
        
        print(f"âœ… í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {n_clusters}ê°œ í´ëŸ¬ìŠ¤í„°")
        
        # ì‹œê°í™”: í´ëŸ¬ìŠ¤í„° ë¶„í¬ (ê°œë³„ ì°¨íŠ¸ë¡œ ì €ì¥)
        if 'cluster_labels' in cluster_cache and 'embeddings_2d' in cluster_cache:
            labels = np.array(cluster_cache['cluster_labels'])
            emb_2d = np.array(cluster_cache['embeddings_2d'])
            
            from collections import Counter
            cluster_counts = Counter(labels)
            
            # 1. í´ëŸ¬ìŠ¤í„° í¬ê¸° ë¶„í¬
            plt.figure(figsize=(10, 6))
            plt.bar(cluster_counts.keys(), cluster_counts.values(), 
                   color='lightcoral', edgecolor='black', alpha=0.7)
            plt.title('Cluster Size Distribution', fontsize=14, fontweight='bold')
            plt.xlabel('Cluster ID')
            plt.ylabel('Count')
            plt.grid(alpha=0.3, axis='y')
            plt.tight_layout()
            plt.savefig(plot_dir / 'cluster_distribution.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            # 2. í´ëŸ¬ìŠ¤í„° ì‹œê°í™” (2D PCA)
            plt.figure(figsize=(10, 8))
            scatter = plt.scatter(emb_2d[:, 0], emb_2d[:, 1], c=labels, 
                                cmap='tab10', alpha=0.6, s=100, edgecolors='black')
            plt.title('Cluster Visualization', fontsize=14, fontweight='bold')
            plt.xlabel('PC1')
            plt.ylabel('PC2')
            plt.colorbar(scatter, label='Cluster ID')
            plt.grid(alpha=0.3)
            plt.tight_layout()
            plt.savefig(plot_dir / 'cluster_visualization.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"   ğŸ“Š í´ëŸ¬ìŠ¤í„° ë¶„ì„ ì‹œê°í™” ì €ì¥: {plot_dir}/*.png (2ê°œ íŒŒì¼)")
    else:
        print("âš ï¸ í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ê²°ê³¼ ì—†ìŒ")
    
    print()
    
    # 4. ë©”íŠ¸ë¦­ ì €ì¥
    metrics["timestamp"] = timestamp
    metrics["dataset_path"] = str(data_dir)
    metrics_file = analysis_root / "metrics.json"
    with open(metrics_file, 'w') as f:
        json.dump(metrics, f, indent=2)
    print(f"ğŸ“ ë©”íŠ¸ë¦­ ì €ì¥: {metrics_file}")
    
    print("=" * 80)
    print(f"âœ… ì „ì²´ ë¶„ì„ ì™„ë£Œ: {timestamp}")
    print(f"   ë°ì´í„°ì…‹: {data_dir}")
    if 'num_files' in metrics:
        print(f"   ì´ íŒŒì¼: {metrics['num_files']}ê°œ")
        print(f"   ìƒˆë¡œ ë¶„ì„: {metrics['files_processed']}ê°œ")
        print(f"   ìºì‹œ í™œìš©: {metrics['files_cached']}ê°œ")

if __name__ == "__main__":
    import sys
    
    # CLI ì¸ìë¡œ ë°ì´í„°ì…‹ ì§€ì • ê°€ëŠ¥
    dataset_name = sys.argv[1] if len(sys.argv) > 1 else None
    
    if dataset_name:
        print(f"ğŸ“¦ ë°ì´í„°ì…‹: {dataset_name}")
    else:
        print("ğŸ“¦ ê¸°ë³¸ ë°ì´í„°ì…‹ ì‚¬ìš©")
    
    analyze_dataset(dataset_name)
