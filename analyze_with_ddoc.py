#!/usr/bin/env python3
"""
DVCì™€ ddoc ëª¨ë“ˆì„ í†µí•©í•œ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
DVCê°€ datasets ë³€ê²½ì„ ê°ì§€í•˜ë©´ ì‹¤í–‰ë˜ê³ , ddocì˜ ìºì‹œë¡œ ì¦ë¶„ ë¶„ì„
"""
import sys
import os
from pathlib import Path
from datetime import datetime
import json
import pickle
import yaml

# ddoc ëª¨ë“ˆ import
sys.path.insert(0, str(Path(__file__).parent.parent / 'datadrift_app_engine'))

try:
    from main import run_attribute_analysis_wrapper, run_embedding_analysis
    from cache_utils import get_cached_analysis_data, save_analysis_data
    from dvclive import Live
    print("âœ… ddoc ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ ddoc ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("datadrift_app_engineì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    sys.exit(1)

def analyze_dataset():
    """ddoc ëª¨ë“ˆë¡œ ë°ì´í„°ì…‹ ë¶„ì„"""
    
    # params.yaml ë¡œë“œ
    with open('params.yaml', 'r') as f:
        params = yaml.safe_load(f)
    
    data_dir = params['analysis']['data_dir']
    formats = tuple(params['analysis']['formats'])
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    output_dir = Path("analysis/current")
    output_dir.mkdir(parents=True, exist_ok=True)
    plot_dir = output_dir / "plots"
    plot_dir.mkdir(exist_ok=True)
    
    print(f"ğŸš€ DVC + ddoc í†µí•© ë¶„ì„ ì‹œì‘")
    print(f"=" * 80)
    print(f"ì‹œê°„: {timestamp}")
    print(f"ë°ì´í„° ë””ë ‰í† ë¦¬: {data_dir}")
    print(f"ì§€ì› í˜•ì‹: {formats}")
    print()
    
    # DVCLiveë¡œ ë©”íŠ¸ë¦­ íŠ¸ë˜í‚¹
    with Live(dir=f"dvclive/analysis_{timestamp}", save_dvc_exp=True) as live:
        live.log_param("timestamp", timestamp)
        live.log_param("data_dir", data_dir)
        
        # 1. ì†ì„± ë¶„ì„ (ddocì˜ í•´ì‹œ ê¸°ë°˜ ìºì‹± í™œìš©)
        print("ğŸ“Š Step 1: Attribute Analysis")
        print("-" * 80)
        
        attr_stats = run_attribute_analysis_wrapper([data_dir], formats)
        
        # ddoc ìºì‹œì—ì„œ ì „ì²´ ê²°ê³¼ ë¡œë“œ
        attr_cache = get_cached_analysis_data(data_dir, "attribute_analysis")
        
        if attr_cache:
            num_files = len(attr_cache)
            sizes = [v['size'] for v in attr_cache.values() if 'size' in v]
            widths = [v['width'] for v in attr_cache.values() if 'width' in v]
            heights = [v['height'] for v in attr_cache.values() if 'height' in v]
            
            # ë©”íŠ¸ë¦­ ë¡œê¹…
            live.log_metric("num_files", num_files)
            live.log_metric("avg_size_mb", sum(sizes) / len(sizes) if sizes else 0)
            live.log_metric("avg_width", sum(widths) / len(widths) if widths else 0)
            live.log_metric("avg_height", sum(heights) / len(heights) if heights else 0)
            live.log_metric("files_processed", attr_stats[data_dir]['processed_files'])
            live.log_metric("files_cached", attr_stats[data_dir]['skipped_files'])
            
            # ê²°ê³¼ ì €ì¥
            with open(output_dir / 'attributes.pkl', 'wb') as f:
                pickle.dump(attr_cache, f)
            
            print(f"âœ… ë¶„ì„ ì™„ë£Œ: {num_files}ê°œ íŒŒì¼")
            print(f"   ìƒˆë¡œ ë¶„ì„: {attr_stats[data_dir]['processed_files']}ê°œ")
            print(f"   ìºì‹œ í™œìš©: {attr_stats[data_dir]['skipped_files']}ê°œ")
            
            # ì‹œê°í™”: íŒŒì¼ í¬ê¸° ë¶„í¬
            import matplotlib.pyplot as plt
            import numpy as np
            
            plt.figure(figsize=(10, 5))
            plt.hist(sizes, bins=20, color='skyblue', edgecolor='black', alpha=0.7)
            plt.title('File Size Distribution', fontsize=14, fontweight='bold')
            plt.xlabel('Size (MB)')
            plt.ylabel('Count')
            plt.grid(alpha=0.3)
            plt.tight_layout()
            plt.savefig(plot_dir / 'size_distribution.png', dpi=300, bbox_inches='tight')
            live.log_image(str(plot_dir / 'size_distribution.png'))
            plt.close()
        else:
            print("âš ï¸ ì†ì„± ë¶„ì„ ê²°ê³¼ ì—†ìŒ")
        
        print()
        
        # 2. ì„ë² ë”© ë¶„ì„ (ddocì˜ í•´ì‹œ ê¸°ë°˜ ìºì‹± í™œìš©)
        print("ğŸ”¬ Step 2: Embedding Analysis")
        print("-" * 80)
        
        emb_stats = run_embedding_analysis(
            [data_dir], 
            formats,
            model=params['embedding']['model'],
            device=params['embedding']['device'],
            n_clusters=params['clustering']['n_clusters'],
            method=params['clustering']['method'],
            cluster_selection_method=params['clustering']['selection_method']
        )
        
        # ddoc ìºì‹œì—ì„œ ì„ë² ë”© ê²°ê³¼ ë¡œë“œ
        emb_cache = get_cached_analysis_data(data_dir, "embedding_analysis")
        
        if emb_cache:
            embeddings = [v['embedding'] for v in emb_cache.values() if 'embedding' in v]
            
            live.log_metric("num_embeddings", len(embeddings))
            live.log_metric("embedding_dim", len(embeddings[0]) if embeddings else 0)
            
            # ê²°ê³¼ ì €ì¥
            with open(output_dir / 'embeddings.pkl', 'wb') as f:
                pickle.dump(emb_cache, f)
            
            print(f"âœ… ì„ë² ë”© ì™„ë£Œ: {len(embeddings)}ê°œ")
            
            # ì‹œê°í™”: PCA 3D
            if len(embeddings) > 1:
                from sklearn.decomposition import PCA
                from mpl_toolkits.mplot3d import Axes3D
                
                emb_array = np.array(embeddings)
                pca = PCA(n_components=3)
                emb_3d = pca.fit_transform(emb_array)
                
                fig = plt.figure(figsize=(12, 9))
                ax = fig.add_subplot(111, projection='3d')
                
                scatter = ax.scatter(emb_3d[:, 0], emb_3d[:, 1], emb_3d[:, 2],
                                    alpha=0.6, s=100, c=range(len(emb_3d)), 
                                    cmap='viridis', edgecolors='black', linewidth=0.5)
                
                ax.set_title(f'Embedding Space (PCA 3D)\nVariance: {pca.explained_variance_ratio_.sum():.1%}', 
                            fontsize=14, fontweight='bold')
                ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
                ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
                ax.set_zlabel(f'PC3 ({pca.explained_variance_ratio_[2]:.1%})')
                
                plt.colorbar(scatter, label='Sample Index', pad=0.1)
                plt.tight_layout()
                plt.savefig(plot_dir / 'embedding_pca_3d.png', dpi=300, bbox_inches='tight')
                live.log_image(str(plot_dir / 'embedding_pca_3d.png'))
                plt.close()
        else:
            print("âš ï¸ ì„ë² ë”© ë¶„ì„ ê²°ê³¼ ì—†ìŒ")
        
        print()
        
        # 3. í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„
        print("ğŸ¯ Step 3: Clustering Analysis")
        print("-" * 80)
        
        cluster_cache = get_cached_analysis_data(data_dir, "clustering_analysis")
        
        if cluster_cache:
            n_clusters = cluster_cache.get('n_clusters', 0)
            
            live.log_metric("num_clusters", n_clusters)
            
            # ê²°ê³¼ ì €ì¥
            with open(output_dir / 'clusters.pkl', 'wb') as f:
                pickle.dump(cluster_cache, f)
            
            print(f"âœ… í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {n_clusters}ê°œ í´ëŸ¬ìŠ¤í„°")
            
            # ì‹œê°í™”: í´ëŸ¬ìŠ¤í„° ë¶„í¬
            if 'cluster_labels' in cluster_cache and 'embeddings_2d' in cluster_cache:
                labels = np.array(cluster_cache['cluster_labels'])
                emb_2d = np.array(cluster_cache['embeddings_2d'])
                
                from collections import Counter
                cluster_counts = Counter(labels)
                
                fig, axes = plt.subplots(1, 2, figsize=(14, 5))
                
                # í´ëŸ¬ìŠ¤í„° í¬ê¸°
                axes[0].bar(cluster_counts.keys(), cluster_counts.values(), 
                           color='lightcoral', edgecolor='black', alpha=0.7)
                axes[0].set_title('Cluster Size Distribution', fontsize=12, fontweight='bold')
                axes[0].set_xlabel('Cluster ID')
                axes[0].set_ylabel('Count')
                axes[0].grid(alpha=0.3, axis='y')
                
                # í´ëŸ¬ìŠ¤í„° ì‹œê°í™”
                scatter = axes[1].scatter(emb_2d[:, 0], emb_2d[:, 1], c=labels, 
                                         cmap='tab10', alpha=0.6, s=100, edgecolors='black')
                axes[1].set_title('Cluster Visualization', fontsize=12, fontweight='bold')
                axes[1].set_xlabel('PC1')
                axes[1].set_ylabel('PC2')
                axes[1].grid(alpha=0.3)
                plt.colorbar(scatter, ax=axes[1], label='Cluster ID')
                
                plt.tight_layout()
                plt.savefig(plot_dir / 'cluster_analysis.png', dpi=300, bbox_inches='tight')
                live.log_image(str(plot_dir / 'cluster_analysis.png'))
                plt.close()
        else:
            print("âš ï¸ í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ê²°ê³¼ ì—†ìŒ")
        
        print()
        
        # 4. ë©”íŠ¸ë¦­ ìš”ì•½ ì €ì¥
        metrics = {
            'timestamp': timestamp,
            'num_files': num_files if 'num_files' in locals() else 0,
            'files_processed': attr_stats[data_dir]['processed_files'],
            'files_cached': attr_stats[data_dir]['skipped_files'],
            'num_embeddings': len(embeddings) if 'embeddings' in locals() else 0,
            'num_clusters': n_clusters if 'n_clusters' in locals() else 0
        }
        
        with open(output_dir / 'metrics.json', 'w') as f:
            json.dump(metrics, f, indent=2)
        
        print("=" * 80)
        print(f"âœ… ì „ì²´ ë¶„ì„ ì™„ë£Œ: {timestamp}")
        print(f"   ì´ íŒŒì¼: {metrics['num_files']}ê°œ")
        print(f"   ìƒˆë¡œ ë¶„ì„: {metrics['files_processed']}ê°œ")
        print(f"   ìºì‹œ í™œìš©: {metrics['files_cached']}ê°œ")

if __name__ == "__main__":
    analyze_dataset()
